{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393caf74-4a65-415d-be15-59b01160918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.widgets import Slider\n",
    "from lab_utils_common import dlc\n",
    "from lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a9293-898e-44e0-b358-1d2ba57660b4",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fa711-dae9-4f0f-9b79-d4e6127d606b",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db03588-3f9a-4c6f-a9a6-2e30d14e996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    sm = np.sum(ez)\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7690096-6bcd-4be5-9a22-5b46527456be",
   "metadata": {},
   "source": [
    "As you are varying the values of the z's above, there are a few things to note:\n",
    "* the exponential in the numerator of the softmax magnifies small differences in the values \n",
    "* the output values sum to one\n",
    "* the softmax spans all of the outputs. A change in `z0` for example will change the values of `a0`-`a3`. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7927fa3-45b6-4025-83c9-4e117c5b44cf",
   "metadata": {},
   "source": [
    "### Softmax + Cross-Entropy Loss (Step-by-Step)\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Softmax Output\n",
    "- Model se output **logits** aate hai → unko softmax se pass karte hai → **probabilities** milti hai.\n",
    "- Softmax formula:\n",
    "\\begin{equation}\n",
    "[\n",
    "a_j = \\frac{e^{z_j}}{\\sum_{k=1}^{N} e^{z_k}}\n",
    "]\n",
    "\\end{equation}\n",
    "- Matlab **model ke hisaab se har class ki probability**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Ek example ke liye loss\n",
    "- Agar true label \\(y = 3\\) hai aur model output \\(\\mathbf{a} = [0.1, 0.2, 0.7]\\) hai:\n",
    "\\begin{equation}\n",
    "[\n",
    "\\text{Loss} = -\\log(a_{\\text{true label}}) = -\\log(0.7)\n",
    "]\n",
    "\\end{equation}\n",
    "- Sirf **true label wali probability** ko consider karte hai, baki ignore.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Indicator Function\n",
    "- **Indicator function**:\n",
    "\\[\n",
    "\\mathbf{1}\\{y=j\\} =\n",
    "\\begin{cases}\n",
    "1 & \\text{agar true label } j \\text{ hai} \\\\\n",
    "0 & \\text{warna ignore}\n",
    "\\end{cases}\n",
    "\\]\n",
    "- Iska use karke ek formula likh sakte hai jo **sab classes par chale par sirf true class pick kare**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Cost Function (batch average)\n",
    "- Ek example ke liye:\n",
    "\\begin{equation}\n",
    "[\n",
    "\\text{Loss} = -\\log \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n",
    "]\n",
    "\\end{equation}\n",
    "- **Batch ke liye average:**\n",
    "$$\n",
    "[\n",
    "J(w,b) = - \\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^N \n",
    "1\\{y^{(i)} = j\\}\n",
    "\\log \\frac{e^{z_j^{(i)}}}{\\sum_k e^{z_k^{(i)}}}\n",
    "]\n",
    "$$\n",
    "- **Indicator** ensure karta hai ki har example ke liye sirf uska true label loss mein add ho.\n",
    "\n",
    "---\n",
    "\n",
    "### **Numerical Example**\n",
    "\n",
    "**Setup:**\n",
    "- Classes (N) = 3 → (Dog, Cat, Horse)  \n",
    "- Batch size (m) = 3 examples  \n",
    "- Logits (z) (model raw outputs):  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2c898-4fe8-4a99-9c51-388761b20c5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 1: Softmax Outputs\n",
    "- Example 1 → Softmax: **[0.66, 0.24, 0.10]**  \n",
    "- Example 2 → Softmax: **[0.10, 0.66, 0.24]**  \n",
    "- Example 3 → Softmax: **[0.24, 0.10, 0.66]**\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Loss for each example\n",
    "$$\n",
    "L_i = -\\log(a_{\\text{true label}})\n",
    "$$\n",
    "- Example 1 → true label = 1 → $-\\log(0.66)=0.415$  \n",
    "- Example 2 → true label = 2 → $-\\log(0.66)=0.415$  \n",
    "- Example 3 → true label = 3 → $-\\log(0.66)=0.415$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Cost (average)\n",
    "$$\n",
    "J = \\frac{1}{m}\\sum_{i=1}^{m}L_i\n",
    "  = \\frac{0.415+0.415+0.415}{3}\n",
    "  = 0.415\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Indicator ka role\n",
    "- Har example ke liye ek **one-hot** indicator hota hai:\n",
    "  - Example 1 → [1,0,0]\n",
    "  - Example 2 → [0,1,0]\n",
    "  - Example 3 → [0,0,1]\n",
    "- Formula mein:\n",
    "$$\n",
    "\\sum_j 1\\{y=i\\} \\log(a_j) \n",
    "\\to \\text{sirf true label pick hota hai.}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line:**  \n",
    "- Softmax se probability nikal  \n",
    "- True label ki probability lo  \n",
    "- $-\\log(\\text{true prob})$ ka average le lo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89e534d-8d68-4cbc-a34d-c25d67bcc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6b800a-c341-4087-a17a-2bee3b000eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2010\n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5391\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2577\n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1435\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0918\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0755\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0611\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0501\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0421\n",
      "Epoch 10/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1958dd257e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(25, activation=\"relu\"),\n",
    "    Dense(15, activation=\"relu\"),\n",
    "    Dense(4, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de0715a-2392-4032-9d0e-dd2b47959c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "16.596172 -7.2160234\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(X_train)\n",
    "print(np.max(output), np.min(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64aeca54-5b98-47b7-950e-320b9a27ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999714 2.86837e-10\n"
     ]
    }
   ],
   "source": [
    "sm_output = tf.nn.softmax(output).numpy()\n",
    "print(np.max(sm_output), np.min(sm_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25e5c24-7c43-4d25-a9ee-04ec3a576337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.98 0.02]\n",
      "[9.94e-01 5.92e-03 7.62e-05 6.85e-06]\n",
      "[9.66e-01 3.36e-02 7.19e-04 1.20e-04]\n",
      "[1.18e-02 9.79e-01 5.20e-04 8.25e-03]\n",
      "[2.48e-03 9.25e-05 9.97e-01 6.35e-05]\n"
     ]
    }
   ],
   "source": [
    "for i in range (5):\n",
    "    print(sm_output[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656957c2-2772-4ebf-a38c-c9c6e26d0479",
   "metadata": {},
   "source": [
    "### SparseCategoricalCrossentropy vs CategoricalCrossentropy\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. SparseCategoricalCrossentropy\n",
    "- **Target format:** Just the class index as an integer.  \n",
    "- **Example:** 10 classes (0–9)  \n",
    "  - If true label is class 2 → `y = 2`  \n",
    "- **Why?** You don’t need to manually make one-hot vectors; TensorFlow will internally convert it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. CategoricalCrossentropy\n",
    "- **Target format:** One-hot vector.  \n",
    "- **Example:** 10 classes (0–9)  \n",
    "  - If true label is class 2 → `y = [0,0,1,0,0,0,0,0,0,0]`  \n",
    "- **Why?** Use when your labels are already one-hot encoded.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual difference\n",
    "\n",
    "| Label Type                  | Example (true class = 2)     |\n",
    "|----------------------------|-----------------------------|\n",
    "| Sparse (integer index)     | `2`                         |\n",
    "| One-hot vector (length=10) | `[0,0,1,0,0,0,0,0,0,0]`     |\n",
    "\n",
    "\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "---\n",
    "\n",
    "#### Use **SparseCategoricalCrossentropy** when:\n",
    "- Your labels are **integers** (e.g., `[2, 0, 5, 3]`).\n",
    "- Your dataset **does not** already have one-hot encoded labels.\n",
    "- **Memory efficient** (no need to store full one-hot vectors).\n",
    "\n",
    "---\n",
    "\n",
    "#### Use **CategoricalCrossentropy** when:\n",
    "- Your labels are **already one-hot encoded** (e.g., `[[0,0,1,0], [1,0,0,0], ...]`).\n",
    "- You are **manually handling one-hot encoding** or working with models that output one-hot format.\n",
    "\n",
    "---\n",
    "\n",
    "### Bottom Line:\n",
    "- **SparseCategoricalCrossentropy → Integer labels**  \n",
    "- **CategoricalCrossentropy → One-hot labels**  \n",
    "- **Loss result is the same**, only input label format differs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b74e44-67a7-4149-8df8-d24dd3580e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
